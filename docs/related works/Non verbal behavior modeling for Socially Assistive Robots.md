# Related Works

## Non verbal behavior modeling for Socially Assistive Robots

- Socially assistive robotics (SAR)

  <img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161452950.png" alt="image-20220608161452950" style="zoom:80%;" />

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161518905.png" alt="image-20220608161518905" style="zoom:80%;" />

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161537482.png" alt="image-20220608161537482" style="zoom:80%;" />

- Focus of research

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161721240.png" alt="image-20220608161721240" style="zoom:80%;" />

- Demand for data-driven robot behavior models

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161800702.png" alt="image-20220608161800702" style="zoom:80%;" />

- Model

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161826189.png" alt="image-20220608161826189" style="zoom:80%;" />

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608161837298.png" alt="image-20220608161837298" style="zoom:50%;" />

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608163042159.png" alt="image-20220608163042159" style="zoom:80%;" />

​				This is similar to our multi-modal system in that we also collect data (hand gestures) 				from users and train a network to recognize the gestures. 

- Data collection

  <img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608163414826.png" alt="image-20220608163414826" style="zoom:80%;" />

​							This data was then manually annotated (for non verbal behavioral features)

- Classification

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608163730968.png" alt="image-20220608163730968" style="zoom:80%;" />

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608163753581.png" alt="image-20220608163753581" style="zoom:80%;" />

- Robot behavior generation

<img src="/home/sam/snap/typora/57/.config/Typora/typora-user-images/image-20220608165053087.png" alt="image-20220608165053087" style="zoom:80%;" />

This paper is similar to ours in the sense that we also try to predict context (but also commands) based on non-verbal input (gestures). Where it differs is of course the type of none-verbal input (they use gaze, gestures, context and objects) as well as they attempt as to generate robot behavior given a context (bi-directional)

## Robot Programming by Demonstration with Interactive Action Visualizations

- Programming by Demonstration (PbD)

<img src="/home/sam/nao-move/docs/related works/image-20220620104641030.png" alt="image-20220620104641030" style="zoom:80%;" />

<img src="/home/sam/nao-move/docs/related works/image-20220620112913178.png" alt="image-20220620112913178" style="zoom:80%;" />

- Similar to our work, demonstrating once and providing feedback 

<img src="/home/sam/nao-move/docs/related works/image-20220620113326248.png" alt="image-20220620113326248" style="zoom:80%;" />

- End user programming (EUP), again similar to our work

  ![image-20220620115144181](/home/sam/nao-move/docs/related works/image-20220620115144181.png)

- System

  <img src="/home/sam/nao-move/docs/related works/image-20220620120753259.png" alt="image-20220620120753259" style="zoom:80%;" />

<img src="/home/sam/nao-move/docs/related works/image-20220620120811326.png" alt="image-20220620120811326" style="zoom:80%;" />

<img src="/home/sam/nao-move/docs/related works/image-20220620120923349.png" alt="image-20220620120923349" style="zoom:80%;" />

- Response to commands

  <img src="/home/sam/nao-move/docs/related works/image-20220620121407473.png" alt="image-20220620121407473" style="zoom:80%;" />	

- Difference: using GUI to edit programmed actions

<img src="/home/sam/nao-move/docs/related works/image-20220620132201579.png" alt="image-20220620132201579" style="zoom:80%;" />

![image-20220620132207713](/home/sam/nao-move/docs/related works/image-20220620132207713.png)

- Evaluation (by authors)

<img src="/home/sam/nao-move/docs/related works/image-20220620134720633.png" alt="image-20220620134720633" style="zoom:80%;" />

- End user evaluation
- Conclusion

<img src="/home/sam/nao-move/docs/related works/image-20220620140031513.png" alt="image-20220620140031513" style="zoom:80%;" />

This work involved demonstrating through physical movement of moving the robot arm to perform certain tasks. A user could modify the movement by interacting with a GUI

## Two-handed gesture recognition and fusion with speech to command a robot

![image-20220620222637935](/home/sam/nao-move/docs/related works/image-20220620222637935.png)

- Intro

  ![image-20220620222653051](/home/sam/nao-move/docs/related works/image-20220620222653051.png)



## The impact of human–robot multimodal communication on mental workload, usability preference, and expectations of robot
behavior



## Gesture and Sign language in HRI (book)

## Other sources

https://economictimes.indiatimes.com/tech/newsletters/ettech-unwrapped/startups-switch-gears-as-vc-large-funding-deals-dry-up/articleshow/91992745.cms?from=mdr

December 2021 by International Federation of Robotics (IFR).

Robotics numbers