{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.flip(image, 1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             #mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                            # mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             #) \n",
    "\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                              mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                              mp_drawing_styles.get_default_hand_connections_style())                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2.364] global /io/opencv/modules/videoio/src/cap_v4l.cpp (889) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(image, results)\n",
    "        print(f'shape: {np.shape(keypoints)}')\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [341]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_hand_landmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmark\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [253]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m nose_landmark_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(nose_landmark\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m image_height), image_height \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m nose_landmarks \u001b[38;5;241m=\u001b[39m [nose_landmark_x, nose_landmark_y]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcalc_landmark_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_hand_landmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnose_landmarks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [247]\u001b[0m, in \u001b[0;36mcalc_landmark_list\u001b[0;34m(image, landmarks, nose_landmarks)\u001b[0m\n\u001b[1;32m      4\u001b[0m landmark_point \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;66;03m# Keypoint\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, landmark \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmark\u001b[49m):\n\u001b[1;32m      8\u001b[0m     landmark_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(landmark\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m*\u001b[39m image_width), image_width \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m     landmark_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mint\u001b[39m(landmark\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m image_height), image_height \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "nose_landmark = results.face_landmarks.landmark[0]\n",
    "image_width, image_height = image.shape[1], image.shape[0]\n",
    "nose_landmark_x = min(int(nose_landmark.x * image_width), image_width - 1)\n",
    "nose_landmark_y = min(int(nose_landmark.y * image_height), image_height - 1)\n",
    "nose_landmarks = [nose_landmark_x, nose_landmark_y]\n",
    "\n",
    "calc_landmark_list(image, results.right_hand_landmarks, nose_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nose_landmark_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.7505885362625122\n",
       "y: 0.40703168511390686\n",
       "z: -0.3763485550880432\n",
       "visibility: 0.9999094009399414"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_lh = np.random.rand(21)\n",
    "calc_rh = np.random.rand(21)\n",
    "pre_lh = np.random.rand(42)\n",
    "pre_rh = np.random.rand(42)\n",
    "nose_landmark_x = 0\n",
    "nose_landmark_y = 0\n",
    "\n",
    "calc_landmark_list(image, results.left_hand_landmarks, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list2(image, landmarks, nose_landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "      # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        #landmark_z = abs(landmark.z)\n",
    "        landmark_point.append([landmark_x, landmark_y, nose_landmarks[0], nose_landmarks[1]])\n",
    "    return landmark_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "      # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        #landmark_z = abs(landmark.z)\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "    return landmark_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_landmark2(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "  \n",
    "      # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "  \n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "        temp_landmark_list[index][2] = temp_landmark_list[index][0] - temp_landmark_list[index][2]\n",
    "        temp_landmark_list[index][3] = temp_landmark_list[index][1] - temp_landmark_list[index][3]\n",
    "      # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "      \n",
    "      # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "  \n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "  \n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "      \n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "  \n",
    "      # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "  \n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "       \n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "      \n",
    "      # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "  \n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "  \n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "      \n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints2(image, results):\n",
    "    calc_lh = np.random.rand(21)\n",
    "    calc_rh = np.random.rand(21)\n",
    "    pre_lh = np.random.rand(84)\n",
    "    pre_rh = np.random.rand(84)\n",
    "    nose_landmark_x = 0\n",
    "    nose_landmark_y = 0\n",
    "    \n",
    "    if results.pose_world_landmarks:\n",
    "        nose_landmark_x = results.pose_world_landmarks.landmark[0].x\n",
    "        nose_landmark_y = results.pose_world_landmarks.landmark[0].y\n",
    "    nose_landmarks = [nose_landmark_x, nose_landmark_y]\n",
    "    if results.left_hand_landmarks:\n",
    "        calc_lh = calc_landmark_list(image, results.left_hand_landmarks, nose_landmarks)\n",
    "        pre_lh = pre_process_landmark(calc_lh)\n",
    "    if results.right_hand_landmarks:\n",
    "        calc_rh = calc_landmark_list(image, results.right_hand_landmarks, nose_landmarks)\n",
    "        pre_rh = pre_process_landmark(calc_rh)\n",
    "\n",
    "    return np.concatenate((np.array(pre_lh), np.array(pre_rh)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(image, results):\n",
    "    calc_lh = np.random.rand(21)\n",
    "    calc_rh = np.random.rand(21)\n",
    "    pre_lh = np.random.rand(42)\n",
    "    pre_rh = np.random.rand(42)\n",
    "    nose_landmark_x = 0\n",
    "    nose_landmark_y = 0\n",
    "    rh_world_x = 0\n",
    "    lh_world_x = 0\n",
    "    rh_world_y = 0\n",
    "    lh_world_y = 0\n",
    "    \n",
    "    if results.pose_world_landmarks:\n",
    "        nose_landmark_x = results.pose_world_landmarks.landmark[0].x\n",
    "        nose_landmark_y = results.pose_world_landmarks.landmark[0].y\n",
    "        rh_world_x = results.pose_world_landmarks.landmark[16].x\n",
    "        lh_world_x = results.pose_world_landmarks.landmark[15].x\n",
    "        rh_world_y = results.pose_world_landmarks.landmark[16].y\n",
    "        lh_world_y = results.pose_world_landmarks.landmark[15].y\n",
    "    nose_landmarks = [nose_landmark_x, nose_landmark_y]\n",
    "    hand_world_landmarks = [rh_world_x, rh_world_y, lh_world_x, lh_world_y]\n",
    "    if results.left_hand_landmarks:\n",
    "        calc_lh = calc_landmark_list(image, results.left_hand_landmarks)\n",
    "        pre_lh = pre_process_landmark(calc_lh)\n",
    "    if results.right_hand_landmarks:\n",
    "        calc_rh = calc_landmark_list(image, results.right_hand_landmarks)\n",
    "        pre_rh = pre_process_landmark(calc_rh)\n",
    "\n",
    "    return np.concatenate((np.array(pre_lh), np.array(pre_rh), nose_landmarks, hand_world_landmarks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints42(image, results):\n",
    "    calc_lh = np.random.rand(21)\n",
    "    calc_rh = np.random.rand(21)\n",
    "    pre_lh = np.random.rand(21)\n",
    "    pre_rh = np.random.rand(21)\n",
    "    nose_landmark_x = 0\n",
    "    nose_landmark_y = 0\n",
    "    rh_world_x = 0\n",
    "    lh_world_x = 0\n",
    "    rh_world_y = 0\n",
    "    lh_world_y = 0\n",
    "    \n",
    "    if results.pose_world_landmarks:\n",
    "        nose_landmark_x = results.pose_world_landmarks.landmark[0].x\n",
    "        nose_landmark_y = results.pose_world_landmarks.landmark[0].y\n",
    "        rh_world_x = results.pose_world_landmarks.landmark[16].x\n",
    "        lh_world_x = results.pose_world_landmarks.landmark[15].x\n",
    "        rh_world_y = results.pose_world_landmarks.landmark[16].y\n",
    "        lh_world_y = results.pose_world_landmarks.landmark[15].y\n",
    "    nose_landmarks = [nose_landmark_x, nose_landmark_y]\n",
    "    hand_world_landmarks = [rh_world_x, rh_world_y, lh_world_x, lh_world_y]\n",
    "    if results.left_hand_landmarks:\n",
    "        calc_lh = calc_landmark_list(image, results.left_hand_landmarks)\n",
    "        pre_lh = pre_process_landmark(calc_lh)\n",
    "    if results.right_hand_landmarks:\n",
    "        calc_rh = calc_landmark_list(image, results.right_hand_landmarks)\n",
    "        pre_rh = pre_process_landmark(calc_rh)\n",
    "\n",
    "    return np.concatenate((np.array(pre_lh), np.array(pre_rh), nose_landmarks, hand_world_landmarks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints42(image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.09836066,  0.16393443, -0.03278689,\n",
       "        0.40983607,  0.09836066,  0.6557377 ,  0.18032787,  0.85245902,\n",
       "        0.32786885,  0.19672131,  0.31147541,  0.60655738,  0.2295082 ,\n",
       "        0.81967213,  0.16393443,  0.96721311,  0.50819672,  0.21311475,\n",
       "        0.44262295,  0.63934426,  0.3442623 ,  0.85245902,  0.24590164,\n",
       "        1.        ,  0.57377049,  0.24590164,  0.50819672,  0.6557377 ,\n",
       "        0.40983607,  0.83606557,  0.32786885,  0.96721311,  0.55737705,\n",
       "        0.31147541,  0.49180328,  0.62295082,  0.39344262,  0.7704918 ,\n",
       "        0.32786885,  0.8852459 ,  0.        ,  0.        ,  0.32786885,\n",
       "       -0.04918033,  0.63934426,  0.06557377,  0.80327869,  0.26229508,\n",
       "        0.8852459 ,  0.44262295,  0.86885246,  0.04918033,  1.        ,\n",
       "        0.44262295,  1.        ,  0.6557377 ,  0.96721311,  0.78688525,\n",
       "        0.7704918 ,  0.1147541 ,  0.90163934,  0.54098361,  0.90163934,\n",
       "        0.7704918 ,  0.86885246,  0.91803279,  0.63934426,  0.21311475,\n",
       "        0.7704918 ,  0.60655738,  0.78688525,  0.81967213,  0.78688525,\n",
       "        0.95081967,  0.49180328,  0.32786885,  0.59016393,  0.62295082,\n",
       "        0.62295082,  0.78688525,  0.6557377 ,  0.90163934,  0.06782401,\n",
       "       -0.63104582,  0.13124937, -0.22393408,  0.24101675, -0.27224943])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.shape(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3835876 ,  0.47759178, -0.77978629, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['nothing', 'hey'])\n",
    "#actions = np.array(['play', 'hey', 'record', 'feedback', 'stop', 'follow', 'nothing',\n",
    "#                   'left', 'right', 'up', 'down', 'move'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 5\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    dirmax = 0\n",
    "    if os.path.exists(DATA_PATH) and len(np.array(os.listdir(DATA_PATH))) == len(actions):\n",
    "        dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(1+dirmax+sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding a new actino to pre-existing dataset\n",
    "action_to_add = 'move'\n",
    "start_folder = 1\n",
    "end_folder = 170\n",
    "for sequence in range(start_folder,end_folder+1):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action_to_add, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_folder = dirmax if dirmax == 1 else dirmax+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "ret, image = cap.read()\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    cont = True\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        if not cont: break\n",
    "        cv2.putText(image, 'New sign get ready for: {}'.format(action.upper()), (120,200), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,29, 22), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, 'Press \"c\" to continue'.format(action.upper()), (120,240), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (255,29, 22), 2, cv2.LINE_AA)\n",
    "    # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        key = cv2.waitKey(100)\n",
    "        while key != ord('c'):\n",
    "            key = cv2.waitKey(10)\n",
    "        cv2.waitKey(2000)\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder, start_folder+no_sequences):\n",
    "            if not cont: break\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "                if not cont: break\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'RESET for sign: {}'.format(action.upper()), (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(800)\n",
    "                    \n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(image, results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    cont = False\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For appending action to dataset\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, image = cap.read()\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    cont = True\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    action = action_to_add\n",
    "    cv2.putText(image, 'New sign get ready for: {}'.format(action.upper()), (120,200), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,29, 22), 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, 'Press \"c\" to continue'.format(action.upper()), (120,240), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,29, 22), 2, cv2.LINE_AA)\n",
    "# Show to screen\n",
    "    cv2.imshow('OpenCV Feed', image)\n",
    "    key = cv2.waitKey(100)\n",
    "    while key != ord('c'):\n",
    "        key = cv2.waitKey(10)\n",
    "    cv2.waitKey(500)\n",
    "    # Loop through sequences aka videos\n",
    "    for sequence in range(start_folder, end_folder+1):\n",
    "        if not cont: break\n",
    "        # Loop through video length aka sequence length\n",
    "        for frame_num in range(sequence_length):\n",
    "            if not cont: break\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # NEW Apply wait logic\n",
    "            if frame_num == 0: \n",
    "                cv2.putText(image, 'RESET for sign: {}'.format(action.upper()), (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "\n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "                cv2.waitKey(850)\n",
    "\n",
    "            else: \n",
    "                cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # NEW Export keypoints\n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                cont = False\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 0, 'hey': 1}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30, 90)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30, 90)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(sequence_length,90)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6866 - categorical_accuracy: 0.5000 - val_loss: 0.6530 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6526 - categorical_accuracy: 1.0000 - val_loss: 0.6199 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6197 - categorical_accuracy: 1.0000 - val_loss: 0.5573 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5577 - categorical_accuracy: 1.0000 - val_loss: 0.4289 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4310 - categorical_accuracy: 1.0000 - val_loss: 0.1868 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1973 - categorical_accuracy: 1.0000 - val_loss: 0.0054 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0098 - categorical_accuracy: 1.0000 - val_loss: 1.7881e-07 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2501e-06 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 10.5871 - val_categorical_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 168.4941 - val_categorical_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 39.4257 - categorical_accuracy: 0.8750 - val_loss: 116.3023 - val_categorical_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 71.9818 - categorical_accuracy: 0.5000 - val_loss: 148.2222 - val_categorical_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 104.7372 - categorical_accuracy: 0.5000 - val_loss: 149.9268 - val_categorical_accuracy: 0.5000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 109.9721 - categorical_accuracy: 0.5000 - val_loss: 99.5657 - val_categorical_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 32.9539 - categorical_accuracy: 0.8750 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f37af998940>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('06131658.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('05221720.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[44,  0],\n",
       "        [ 1,  3]],\n",
       "\n",
       "       [[40,  1],\n",
       "        [ 0,  7]],\n",
       "\n",
       "       [[41,  1],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[44,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[40,  0],\n",
       "        [ 0,  8]],\n",
       "\n",
       "       [[40,  0],\n",
       "        [ 0,  8]],\n",
       "\n",
       "       [[44,  0],\n",
       "        [ 0,  4]],\n",
       "\n",
       "       [[41,  0],\n",
       "        [ 0,  7]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245), (123,85,25), (11,42,180), (51,142,50), (75,75,50), \n",
    "          (200, 150, 200), (42, 42, 42), (80, 12, 200), (22, 200, 10), (255, 1, 225), (10, 180, 180)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m18\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mprob_viz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36mprob_viz\u001b[0;34m(res, actions, input_frame, colors)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(res):\n\u001b[1;32m      6\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(output_frame, (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), (\u001b[38;5;28mint\u001b[39m(prob\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m), \u001b[38;5;241m90\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), colors[num], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(output_frame, \u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m]\u001b[49m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m85\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_frame\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x1296 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.65\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(image, results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "        \n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
    "#             print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
